{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a070bf",
   "metadata": {},
   "source": [
    "# Uso de MLFLOW\n",
    "En el notebook anterior, hicimos la exploraci칩n de los datos de un dataset de rese침as de videojuegos a partir del cual hice la pr치ctica del m칩dulo NLP de KC. \n",
    "En este caso, vamos a cargar los modelos ya hechos en este cuaderno para cargar estos modelos en MLFLOW y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a253abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones modelo 1: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# rutas del modelo 1\n",
    "ruta_modelo_1 = \"modelos/Logistic_regression_tf_idf0-1/model.pkl\"\n",
    "ruta_vectorizer_1 = \"modelos/Logistic_regression_tf_idf0-1/vectorizer.pkl\"\n",
    "\n",
    "# cargamos modelo y vectorizador\n",
    "modelo_1 = joblib.load(ruta_modelo_1)\n",
    "vectorizer_1 = joblib.load(ruta_vectorizer_1)\n",
    "\n",
    "# datos de prueba\n",
    "data = {'text': ['I love this product!', 'This is the worst service ever.']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# transformaci칩n + predicci칩n\n",
    "X_1 = vectorizer_1.transform(df['text'])\n",
    "pred_1 = modelo_1.predict(X_1)\n",
    "\n",
    "print(\"Predicciones modelo 1:\", pred_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70c1002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones modelo 2: [5 1]\n"
     ]
    }
   ],
   "source": [
    "# rutas del modelo 2\n",
    "ruta_modelo_2 = \"modelos/Logistic_regression_tf_idf0-5/model.pkl\"\n",
    "ruta_vectorizer_2 = \"modelos/Logistic_regression_tf_idf0-5/vectorizer.pkl\"\n",
    "\n",
    "# cargamos modelo y vectorizador\n",
    "modelo_2 = joblib.load(ruta_modelo_2)\n",
    "vectorizer_2 = joblib.load(ruta_vectorizer_2)\n",
    "\n",
    "# transformaci칩n + predicci칩n\n",
    "X_2 = vectorizer_2.transform(df['text'])\n",
    "pred_2 = modelo_2.predict(X_2)\n",
    "\n",
    "print(\"Predicciones modelo 2:\", pred_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada7abb",
   "metadata": {},
   "source": [
    "Tras hacer esta primera carga, observamos dos cosas: \n",
    "- El modelo debe cargarse junto con el vectorizer\n",
    "- VScode nos avisa de que el venv tiene versiones de sus librer칤as distintas a aquellas con las que se cre칩 el modelo. Por ello, vamos a crear un venv en Anaconda con las dependencias exactas para evitar \n",
    "- al hacer un predict sobre data, al tener una lista con 2 rese침as artificiales, nos devuelve una lista con las dos predicciones.\n",
    "        - 'I love this product!' tiene una puntuaci칩n 1, la m치s alta\n",
    "        - 'This is the worst service ever.' tiene una puntuaci칩n 0, la m치s baja\n",
    "    - En el modelo 2, el resultado devuelto es [5,1], por lo que:\n",
    "        - 'I love this product!' tiene una puntuaci칩n 5, la m치s alta\n",
    "        - 'This is the worst service ever.' tiene una puntuaci칩n 1, la m치s baja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102c8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "MLFLOW_TRACKING_URI = 'http://localhost:5000'\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef31f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para conseguir estad칤sticas reales de los modelos, traemos el dataset ya preprocesado y lo dividimos \n",
    "from sklearn.model_selection import train_test_split\n",
    "ruta_dataset_1 = \"reviews_Video_Games_5_balanced_preprocessed_0-1.csv\"\n",
    "ruta_dataset_2 = \"reviews_Video_Games_5_balanced_preprocessed.csv\"\n",
    "df_1 = pd.read_csv(ruta_dataset_1)\n",
    "df_2 = pd.read_csv(ruta_dataset_2)\n",
    "X_1_full = vectorizer_1.transform(df_1['reviewText'])\n",
    "y_1_full = df_1['overall']\n",
    "X_2_full = vectorizer_2.transform(df_2['reviewText'])\n",
    "y_2_full = df_2['overall']\n",
    "# Vamos a dividir en train y test, usando el mismo random state para ambos modelos que usamos al entrenarlos durante la pr치ctica para reproducir los mismos resultados\n",
    "\n",
    "X_1_train, X_1_test, y_1_train, y_1_test = train_test_split(X_1_full, y_1_full, test_size=0.2, random_state=42)\n",
    "X_2_train, X_2_test, y_2_train, y_2_test = train_test_split(X_2_full, y_2_full, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda7a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test Modelo 1: 0.79625, Accuracy Train Modelo 1: 0.91\n",
      "Accuracy Test Modelo 2: 0.416, Accuracy Train Modelo 2: 0.778\n"
     ]
    }
   ],
   "source": [
    "accuracy_test_1 = modelo_1.score(X_1_test, y_1_test); accuracy_test_1\n",
    "accuracy_train_1 = modelo_1.score(X_1_train, y_1_train); accuracy_train_1\n",
    "accuracy_test_2 = modelo_2.score(X_2_test, y_2_test); accuracy_test_2\n",
    "accuracy_train_2 = modelo_2.score(X_2_train, y_2_train); accuracy_train_2\n",
    "\n",
    "print(f\"Accuracy Test Modelo 1: {accuracy_test_1}, Accuracy Train Modelo 1: {accuracy_train_1}\")\n",
    "print(f\"Accuracy Test Modelo 2: {accuracy_test_2}, Accuracy Train Modelo 2: {accuracy_train_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6929edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 13:07:48 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/29 13:07:48 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1769684646051, experiment_id='1', last_update_time=1769684646051, lifecycle_stage='active', name='sentiment-analysis-logreg', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.set_experiment(\"sentiment-analysis-logreg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3040477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 13:27:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游끢 View run LogReg_TFIDF_0.1 at: http://localhost:5000/#/experiments/1/runs/248f549fbb3748d4911c61b382d295b4\n",
      "游빍 View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"LogReg_TFIDF_0.1\"):\n",
    "# Guardamos m칠tricas, par치metros, modelo y vectorizador del modelo 1\n",
    "    mlflow.log_metric(\"accuracy_train\", accuracy_train_1)\n",
    "    mlflow.log_metric(\"accuracy_test\", accuracy_test_1)\n",
    "# Guardamos par치metros\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"tfidf_max_df\", 0.1)\n",
    "# Guardamos modelo \n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=modelo_1,\n",
    "        artifact_path=\"logreg_tfidf_0_1\"\n",
    "    )\n",
    "# Guardamos vectorizador\n",
    "    mlflow.log_artifact(\n",
    "        \"modelos/Logistic_regression_tf_idf0-1/vectorizer.pkl\",\n",
    "        artifact_path=\"vectorizer_tfidf_0_1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c15e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 13:27:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游끢 View run LogReg_TFIDF_0.5 at: http://localhost:5000/#/experiments/1/runs/77ba1d07e08e4ba88b35281416b499fa\n",
      "游빍 View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "#Hacemos lo mismo con el segundo modelo\n",
    "with mlflow.start_run(run_name=\"LogReg_TFIDF_0.5\"):\n",
    "\n",
    "    # m칠tricas\n",
    "    mlflow.log_metric(\"accuracy_train\", accuracy_train_2)\n",
    "    mlflow.log_metric(\"accuracy_test\", accuracy_test_2)\n",
    "\n",
    "    # par치metros\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"tfidf_max_df\", 0.5)\n",
    "\n",
    "    #  modelo\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=modelo_2,\n",
    "        artifact_path=\"logreg_tfidf_0_5\"\n",
    "    )\n",
    "\n",
    "    # vectorizer\n",
    "    mlflow.log_artifact(\n",
    "        \"modelos/Logistic_regression_tf_idf0-5/vectorizer.pkl\",\n",
    "        artifact_path=\"vectorizer_tfidf_0_5\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efc895",
   "metadata": {},
   "source": [
    "Tras algunas ejecuciones, he comprobado que todos los modelos estaban siendo guardados bajo el mismo nombre (model), por lo que he cambiado el par치metro necesario (artifact_path) para tener una pantalla de m칠tricas comprensible, m치s declarativa y clara "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
