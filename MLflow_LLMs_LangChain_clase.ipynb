{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Tracing para LLMs con LangChain y Google Gemini\n",
    "\n",
    "En este notebook exploraremos las nuevas capacidades de **MLflow Tracing** para el seguimiento y observabilidad de modelos de lenguaje (LLMs). \n",
    "\n",
    "## ¿Qué es MLflow Tracing?\n",
    "\n",
    "MLflow Tracing es un sistema de observabilidad que captura automáticamente:\n",
    "- **Prompts y respuestas** enviados/recibidos del modelo\n",
    "- **Latencias** de cada llamada\n",
    "- **Tokens utilizados** (input, output, total)\n",
    "- **Metadatos** (temperatura, modelo, etc.)\n",
    "- **Excepciones** si ocurren errores\n",
    "\n",
    "## Integraciones Soportadas\n",
    "\n",
    "MLflow soporta **40+ integraciones** incluyendo:\n",
    "- **Frameworks de Agentes:** LangChain, LangGraph, DSPy, CrewAI, LlamaIndex\n",
    "- **Proveedores de Modelos:** OpenAI, Anthropic, Google Gemini, Mistral, Ollama\n",
    "\n",
    "## ¿Qué aprenderemos?\n",
    "\n",
    "1. Usar `mlflow.gemini.autolog()` para tracing directo con Google GenAI SDK\n",
    "2. Usar `mlflow.langchain.autolog()` para tracing con LangChain\n",
    "3. Visualizar trazas en la UI de MLflow\n",
    "4. Comparar diferentes configuraciones de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "Instalamos las librerías necesarias. Nota: Se requiere MLflow >= 2.14.0 para tracing con LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow>=2.14.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: langchain in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.0)\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.6-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.1)\n",
      "Requirement already satisfied: mlflow-skinny==3.8.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (3.8.1)\n",
      "Requirement already satisfied: mlflow-tracing==3.8.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (3.8.1)\n",
      "Requirement already satisfied: Flask-CORS<7 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (6.0.2)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (1.18.1)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (46.0.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (3.4.3)\n",
      "Requirement already satisfied: huey<3,>=2.5.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (2.6.0)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (2.3.1)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (22.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (1.7.1)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (1.15.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (2.0.46)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow>=2.14.0) (3.0.2)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (6.2.5)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (8.3.1)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (3.1.2)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.80.0)\n",
      "Requirement already satisfied: fastapi<1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.128.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (3.1.46)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (8.7.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (1.39.1)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (6.33.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (2.32.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.5.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (4.14.1)\n",
      "Requirement already satisfied: uvicorn<1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.40.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from alembic!=1.10.0,<2->mlflow>=2.14.0) (1.3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click<9,>=7.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cryptography<47,>=43.0.0->mlflow>=2.14.0) (2.0.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (2.48.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from docker<8,>=4.0.0->mlflow>=2.14.0) (311)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from docker<8,>=4.0.0->mlflow>=2.14.0) (2.5.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.0.4)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask<4->mlflow>=2.14.0) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask<4->mlflow>=2.14.0) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask<4->mlflow>=2.14.0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask<4->mlflow>=2.14.0) (2.1.5)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask<4->mlflow>=2.14.0) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=2.14.0) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=2.14.0) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (4.9.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphene<4->mlflow>=2.14.0) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphene<4->mlflow>=2.14.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphene<4->mlflow>=2.14.0) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4->mlflow>=2.14.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4->mlflow>=2.14.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4->mlflow>=2.14.0) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4->mlflow>=2.14.0) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4->mlflow>=2.14.0) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4->mlflow>=2.14.0) (3.2.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.60b1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3->mlflow>=2.14.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3->mlflow>=2.14.0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.14.0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow>=2.14.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow>=2.14.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow>=2.14.0) (2025.6.15)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn<2->mlflow>=2.14.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn<2->mlflow>=2.14.0) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.14.0) (3.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.14.0) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.14.0) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow>=2.14.0) (0.16.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.2.2)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.5.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-genai<2.0.0,>=1.56.0 (from langchain-google-genai)\n",
      "  Downloading google_genai-1.60.0-py3-none-any.whl.metadata (53 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n",
      "  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.29.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.188.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.67.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.8.1->mlflow>=2.14.0)\n",
      "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rammu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=2.14.0) (2.22)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.31.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.3.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_genai-1.60.0-py3-none-any.whl (719 kB)\n",
      "   ---------------------------------------- 0.0/719.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 719.4/719.4 kB 4.2 MB/s  0:00:00\n",
      "Downloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Downloading google_generativeai-0.8.6-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 4.5 MB/s  0:00:00\n",
      "Downloading google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.27.0-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading google_api_python_client-2.188.0-py3-none-any.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.9 MB 4.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 4.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.6/14.9 MB 4.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/14.9 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.7/14.9 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.8/14.9 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.8/14.9 MB 4.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.9/14.9 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.9/14.9 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.7/14.9 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.7/14.9 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.1/14.9 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.1/14.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.9/14.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 4.9 MB/s  0:00:03\n",
      "Downloading google_auth_httplib2-0.3.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading httplib2-0.31.2-py3-none-any.whl (91 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: filetype, websockets, uritemplate, protobuf, httplib2, proto-plus, googleapis-common-protos, grpcio-status, langchain-core, google-auth-httplib2, google-api-core, google-genai, google-api-python-client, langchain-google-genai, google-ai-generativelanguage, google-generativeai\n",
      "\n",
      "   ----------------------------------------  0/16 [filetype]\n",
      "   -- -------------------------------------  1/16 [websockets]\n",
      "   -- -------------------------------------  1/16 [websockets]\n",
      "   -- -------------------------------------  1/16 [websockets]\n",
      "   -- -------------------------------------  1/16 [websockets]\n",
      "   -- -------------------------------------  1/16 [websockets]\n",
      "   ----- ----------------------------------  2/16 [uritemplate]\n",
      "  Attempting uninstall: protobuf\n",
      "   ----- ----------------------------------  2/16 [uritemplate]\n",
      "    Found existing installation: protobuf 6.33.0\n",
      "   ----- ----------------------------------  2/16 [uritemplate]\n",
      "    Uninstalling protobuf-6.33.0:\n",
      "   ----- ----------------------------------  2/16 [uritemplate]\n",
      "      Successfully uninstalled protobuf-6.33.0\n",
      "   ----- ----------------------------------  2/16 [uritemplate]\n",
      "   ------- --------------------------------  3/16 [protobuf]\n",
      "   ------- --------------------------------  3/16 [protobuf]\n",
      "   ------- --------------------------------  3/16 [protobuf]\n",
      "   ------- --------------------------------  3/16 [protobuf]\n",
      "   ------- --------------------------------  3/16 [protobuf]\n",
      "   ------- --------------------------------  3/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [proto-plus]\n",
      "   ------------ ---------------------------  5/16 [proto-plus]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   --------------- ------------------------  6/16 [googleapis-common-protos]\n",
      "   ----------------- ----------------------  7/16 [grpcio-status]\n",
      "  Attempting uninstall: langchain-core\n",
      "   ----------------- ----------------------  7/16 [grpcio-status]\n",
      "    Found existing installation: langchain-core 1.2.2\n",
      "   ----------------- ----------------------  7/16 [grpcio-status]\n",
      "    Uninstalling langchain-core-1.2.2:\n",
      "   ----------------- ----------------------  7/16 [grpcio-status]\n",
      "      Successfully uninstalled langchain-core-1.2.2\n",
      "   ----------------- ----------------------  7/16 [grpcio-status]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   -------------------- -------------------  8/16 [langchain-core]\n",
      "   ------------------------- -------------- 10/16 [google-api-core]\n",
      "   ------------------------- -------------- 10/16 [google-api-core]\n",
      "   ------------------------- -------------- 10/16 [google-api-core]\n",
      "   ------------------------- -------------- 10/16 [google-api-core]\n",
      "   ------------------------- -------------- 10/16 [google-api-core]\n",
      "   ------------------------- -------------- 10/16 [google-api-core]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   --------------------------- ------------ 11/16 [google-genai]\n",
      "   ------------------------------ --------- 12/16 [google-api-python-client]\n",
      "   ------------------------------ --------- 12/16 [google-api-python-client]\n",
      "   ------------------------------ --------- 12/16 [google-api-python-client]\n",
      "   ------------------------------ --------- 12/16 [google-api-python-client]\n",
      "   -------------------------------- ------- 13/16 [langchain-google-genai]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 14/16 [google-ai-generativelanguage]\n",
      "   ------------------------------------- -- 15/16 [google-generativeai]\n",
      "   ------------------------------------- -- 15/16 [google-generativeai]\n",
      "   ------------------------------------- -- 15/16 [google-generativeai]\n",
      "   ------------------------------------- -- 15/16 [google-generativeai]\n",
      "   ------------------------------------- -- 15/16 [google-generativeai]\n",
      "   ------------------------------------- -- 15/16 [google-generativeai]\n",
      "   ---------------------------------------- 16/16 [google-generativeai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.15 google-api-core-2.29.0 google-api-python-client-2.188.0 google-auth-httplib2-0.3.0 google-genai-1.60.0 google-generativeai-0.8.6 googleapis-common-protos-1.72.0 grpcio-status-1.71.2 httplib2-0.31.2 langchain-core-1.2.7 langchain-google-genai-4.2.0 proto-plus-1.27.0 protobuf-5.29.5 uritemplate-4.2.0 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script filetype.exe is installed in 'C:\\Users\\rammu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script websockets.exe is installed in 'C:\\Users\\rammu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las dependencias necesarias\n",
    "!pip install \"mlflow>=2.14.0\" langchain langchain-google-genai google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import mlflow \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_TRACKING_URI = 'http://localhost:5000'\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 1: MLflow Tracing con Google GenAI SDK Directo\n",
    "\n",
    "Primero veremos cómo usar `mlflow.gemini.autolog()` para tracing automático con el SDK nativo de Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julen\\anaconda3\\envs\\ia4-mlops\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\julen\\AppData\\Local\\Temp\\ipykernel_11064\\764567260.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai \n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-2.5-flash-lite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('Puedes explicarme las aplicaciones que tiene mlflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Claro! MLflow es una plataforma de código abierto que te ayuda a gestionar todo el ciclo de vida del machine learning. Sus aplicaciones son muy amplias y se centran en resolver los desafíos comunes que enfrentan los científicos de datos e ingenieros de ML.\\n\\nAquí te explico las principales aplicaciones de MLflow, divididas por sus componentes principales:\\n\\n**1. MLflow Tracking:**\\n\\nEsta es la aplicación más fundamental de MLflow y se centra en **registrar, visualizar y comparar experimentos de ML**.\\n\\n*   **Seguimiento de Experimentos de ML:**\\n    *   **Registro de Métricas y Parámetros:** Guarda automáticamente todas las métricas de rendimiento (precisión, F1-score, RMSE, etc.) y los hiperparámetros utilizados en cada ejecución de un modelo.\\n    *   **Registro de Artefactos:** Guarda los modelos entrenados, archivos de preprocesamiento, visualizaciones, y cualquier otro archivo relevante generado durante el experimento.\\n    *   **Visualización de Resultados:** La interfaz de usuario de MLflow permite comparar fácilmente diferentes ejecuciones (experimentos) lado a lado, ver la evolución de las métricas y los valores de los parámetros, lo que facilita la identificación de las configuraciones óptimas.\\n    *   **Reproducibilidad:** Al registrar todos los parámetros, métricas y artefactos, MLflow permite reproducir exactamente un experimento en el futuro, lo cual es crucial para la auditoría y la depuración.\\n    *   **Colaboración:** Permite a los equipos compartir los resultados de sus experimentos de manera centralizada, fomentando la colaboración y evitando duplicar esfuerzos.\\n\\n*   **Ejemplos de Uso:**\\n    *   Un equipo de científicos de datos está probando diferentes algoritmos de clasificación. MLflow les permite registrar los resultados de cada algoritmo con diferentes hiperparámetros, para luego comparar cuál funciona mejor.\\n    *   Un investigador está desarrollando un modelo de detección de anomalías. MLflow rastrea cada iteración del entrenamiento, guardando los modelos intermedios y las métricas de rendimiento, para poder volver a un punto específico si es necesario.\\n\\n**2. MLflow Projects:**\\n\\nEsta aplicación se enfoca en **empaquetar código de ML de manera reproducible y portable**.\\n\\n*   **Empaquetado de Código:** Permite definir un formato estándar para estructurar tus proyectos de ML (usando un archivo `MLproject`). Esto incluye especificar las dependencias (Python, bibliotecas, etc.) y definir cómo ejecutar el código (comandos, argumentos).\\n*   **Ejecución Remota y en Entornos Diversos:** Una vez empaquetado, un proyecto de MLflow puede ser ejecutado en diferentes entornos, ya sea localmente, en clusters (como Spark, Kubernetes) o en la nube. Esto abstrae la complejidad de la configuración del entorno.\\n*   **Control de Versiones del Código:** Al vincular los proyectos de MLflow con sistemas de control de versiones como Git, MLflow garantiza que se pueda ejecutar una versión específica del código en el futuro.\\n\\n*   **Ejemplos de Uso:**\\n    *   Compartir un script de entrenamiento de modelo con colegas. Al empacarlo con MLflow, te aseguras de que puedan ejecutarlo sin preocuparse por instalar las mismas versiones de bibliotecas.\\n    *   Ejecutar un experimento de entrenamiento en un clúster de Spark. MLflow se encarga de empaquetar el código y desplegarlo en el clúster.\\n\\n**3. MLflow Models:**\\n\\nEsta aplicación se centra en **la estandarización y el empaquetado de modelos de ML para su despliegue**.\\n\\n*   **Formato de Modelo Universal:** Define un formato estándar (`MLmodel`) que permite almacenar un modelo de una manera agnóstica al framework (TensorFlow, PyTorch, scikit-learn, XGBoost, etc.). Esto facilita el cambio de un framework a otro o el uso de modelos entrenados en diferentes plataformas.\\n*   **Despliegue Simplificado:** Una vez que un modelo está en el formato de MLflow, se puede desplegar fácilmente en una variedad de entornos, como:\\n    *   **Servidores de Inferencias REST:** Para obtener predicciones en tiempo real.\\n    *   **Plataformas de Big Data:** Como Spark, para realizar predicciones a escala.\\n    *   **Plataformas de despliegue:** Como SageMaker, Azure ML, etc.\\n*   **Versiones de Modelos:** Permite versionar los modelos desplegados, facilitando el roll-back a versiones anteriores si surge algún problema.\\n\\n*   **Ejemplos de Uso:**\\n    *   Un modelo entrenado en PyTorch necesita ser desplegado como un servicio web. MLflow Models permite empaquetarlo de forma que pueda ser servido por un servidor REST genérico.\\n    *   Comparar dos versiones de un modelo desplegado para determinar cuál ofrece un mejor rendimiento en producción.\\n\\n**4. MLflow Registry (Model Registry):**\\n\\nEsta aplicación proporciona una **gestión centralizada de los modelos a lo largo de su ciclo de vida, desde la experimentación hasta la producción**.\\n\\n*   **Ciclo de Vida del Modelo:** Permite definir etapas para los modelos, como \"Staging\" (preparación para producción), \"Production\" (en uso activo) y \"Archived\" (descontinuado).\\n*   **Gestión de Versiones de Modelos:** Mantiene un historial de todas las versiones de un modelo, junto con sus metadatos y las ejecuciones de MLflow que las generaron.\\n*   **Aprobación y Despliegue:** Facilita la revisión y aprobación de modelos antes de pasarlos a producción, estableciendo flujos de trabajo.\\n*   **Gestión de Dependencias:** Asegura que las dependencias del modelo se registren para garantizar que pueda ser desplegado y ejecutado correctamente.\\n\\n*   **Ejemplos de Uso:**\\n    *   Un equipo tiene varios modelos en \"Staging\" que necesitan ser probados en un entorno de pre-producción antes de ser desplegados a producción. El Registry ayuda a gestionar este proceso.\\n    *   Un administrador necesita saber qué versión de un modelo específico está actualmente en producción y quién la desplegó. El Registry proporciona esta información.\\n\\n**En resumen, MLflow te ayuda a:**\\n\\n*   **Organizar y seguir tus experimentos:** Saber qué funcionó, por qué y cómo reproducirlo.\\n*   **Reproducir tu trabajo:** Asegurar que tus resultados sean confiables y auditables.\\n*   **Compartir tu trabajo:** Facilitar la colaboración en equipos.\\n*   **Empaquetar y desplegar modelos fácilmente:** Hacer que tus modelos sean accesibles para la inferencia y la producción.\\n*   **Gestionar el ciclo de vida completo de tus modelos:** Desde la idea hasta la producción y el mantenimiento.\\n\\nMLflow es una herramienta muy valiosa para cualquier persona o equipo que trabaje seriamente en proyectos de machine learning, ya que aborda muchos de los puntos débiles comunes en el desarrollo y despliegue de modelos.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/26 21:40:15 WARNING mlflow.utils.autologging_utils: MLflow gemini autologging is known to be compatible with 0.8.0 <= google-generativeai <= 0.8.3, but the installed version is 0.8.6. If you encounter errors during autologging, try upgrading / downgrading google-generativeai to a compatible version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "mlflow.gemini.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "\n",
    "response = chat.send_message('Explicame cuando salio el modelo text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El modelo `text-davinci-003` fue lanzado por OpenAI en **noviembre de 2022**.\\n\\nFue, en su momento, el modelo más avanzado y potente de la serie GPT-3 disponible a través de la API de OpenAI. Se destacó por su capacidad para:\\n\\n*   Seguir instrucciones complejas de manera más efectiva.\\n*   Generar texto más coherente, creativo y de mayor calidad.\\n*   Realizar tareas de razonamiento, escritura de código y comprensión de lenguaje natural de manera impresionante.\\n\\n`text-davinci-003` jugó un papel fundamental en el auge de la IA generativa y fue el modelo de facto para muchas aplicaciones y experimentos antes de que modelos como `gpt-3.5-turbo` y `gpt-4` fueran lanzados y lo superaran en eficiencia, coste y capacidad.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 2: MLflow Tracing con LangChain\n",
    "\n",
    "Ahora veremos cómo usar `mlflow.langchain.autolog()` para tracing automático con LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Habilitamos el auto-tracing para LangChain\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Configuramos el experimento\n",
    "#mlflow.set_experiment(\"Langchain-ia4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LangChain inicializado\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos el modelo de LangChain\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"Modelo LangChain inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Llamada Simple con Mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "Ah, *mon cher ami*! ¡Qué excelente pregunta! Gestionar un sistema Kubernetes, ¡eso es un arte, una ciencia, una verdadera aventura! Permíteme guiarte a través de este fascinante mundo, con un toque de mi *je ne sais quoi* francés.\n",
      "\n",
      "Kubernetes, como sabes, es el orquestador de contenedores por excelencia. Su poder reside en su capacidad para automatizar la implementación, el escalado y la gestión de aplicaciones en contenedores. Pero, como cualquier sistema complejo, requiere una gestión cuidadosa y estratégica.\n",
      "\n",
      "Aquí tienes los pilares fundamentales para gestionar tu sistema Kubernetes de manera efectiva:\n",
      "\n",
      "### 1. Arquitectura y Diseño: La Base Sólida\n",
      "\n",
      "Antes de empezar a desplegar, ¡piensa en la arquitectura!\n",
      "\n",
      "*   **Clúster:** Decide la topología de tu clúster. ¿Será un clúster pequeño para desarrollo, uno mediano para producción, o un gigante para cargas de trabajo masivas? Considera la separación de nodos de control (control plane) y nodos de trabajo (worker nodes) para mayor estabilidad.\n",
      "*   **Redes:** ¡Ah, la red! Es crucial. Elige una solución de CNI (Container Network Interface) que se adapte a tus necesidades (Calico, Flannel, Cilium, etc.). Piensa en cómo se comunicarán tus pods entre sí y con el exterior. Los `Services` y `Ingress` serán tus mejores amigos aquí.\n",
      "*   **Almacenamiento:** Las aplicaciones a menudo necesitan persistencia. Selecciona tus `StorageClasses` y `PersistentVolumes` adecuadamente. ¿Necesitas almacenamiento de alto rendimiento, replicado, o algo más simple?\n",
      "*   **Seguridad:** ¡Fundamental! Configura `RBAC` (Role-Based Access Control) para definir quién puede hacer qué. Implementa `NetworkPolicies` para controlar el tráfico entre pods. Considera `Secrets` para gestionar información sensible.\n",
      "\n",
      "### 2. Despliegue y Gestión de Aplicaciones: El Corazón del Sistema\n",
      "\n",
      "Aquí es donde tus aplicaciones cobran vida.\n",
      "\n",
      "*   **`Deployments` y `StatefulSets`:** Son tus caballos de batalla. Usa `Deployments` para aplicaciones stateless y `StatefulSets` para aquellas que requieren identidad de red estable y almacenamiento persistente.\n",
      "*   **`Pods`:** La unidad atómica de Kubernetes. Entiende su ciclo de vida y cómo se escalan.\n",
      "*   **`Services`:** Expón tus aplicaciones de forma fiable. `ClusterIP`, `NodePort`, `LoadBalancer`... elige el adecuado.\n",
      "*   **`Ingress`:** Para la gestión del tráfico HTTP/HTTPS entrante. Configura tus `Ingress Controllers` (Nginx, Traefik, HAProxy) para enrutar el tráfico a los servicios correctos.\n",
      "*   **`Helm`:** ¡Mi favorito para la gestión de aplicaciones! `Helm` es el gestor de paquetes para Kubernetes. Te permite definir, instalar y actualizar aplicaciones complejas de manera reproducible y versionada. ¡Es como tener un chef personal para tus despliegues!\n",
      "\n",
      "### 3. Observabilidad: Ver lo que Ocurre\n",
      "\n",
      "No puedes gestionar lo que no puedes ver.\n",
      "\n",
      "*   **Logging:** Centraliza tus logs. Soluciones como EFK (Elasticsearch, Fluentd, Kibana) o PLG (Prometheus, Loki, Grafana) son muy populares. Asegúrate de que tus contenedores expongan sus logs de manera estándar.\n",
      "*   **Monitoring:** Mide el rendimiento de tu clúster y tus aplicaciones. `Prometheus` es el estándar de facto para métricas. Configura `Alertmanager` para recibir notificaciones cuando algo va mal.\n",
      "*   **Tracing:** Para entender el flujo de las peticiones a través de tus microservicios. Soluciones como Jaeger o Zipkin son excelentes.\n",
      "\n",
      "### 4. Gestión del Ciclo de Vida del Clúster: Mantenerlo Fresco y Seguro\n",
      "\n",
      "Tu clúster no es estático.\n",
      "\n",
      "*   **Actualizaciones:** Mantén tu clúster y sus componentes actualizados. Esto es crucial para la seguridad y para acceder a nuevas funcionalidades. Planifica cuidadosamente las actualizaciones del plano de control y de los nodos de trabajo.\n",
      "*   **Escalado:** Escala tu clúster según la demanda. El `Cluster Autoscaler` puede ayudarte a añadir o eliminar nodos automáticamente.\n",
      "*   **Backup y Recuperación:** ¡No te olvides de esto! Haz copias de seguridad de la configuración de tu clúster (etcd) y de tus datos persistentes. Ten un plan de recuperación ante desastres.\n",
      "\n",
      "### 5. Herramientas Esenciales: Tus Fieles Compañeros\n",
      "\n",
      "*   **`kubectl`:** La interfaz de línea de comandos indispensable. ¡Domínala!\n",
      "*   **Dashboard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Eres un experto en LLMOPs con un acento frances\"),\n",
    "    HumanMessage(content=\"Explicame como puedo gestionar un sistema de kubernetes\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"Respuesta:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Usando Chains con PromptTemplate\n",
    "\n",
    "Las chains de LangChain son trazadas automáticamente, capturando cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain creada: PromptTemplate -> LLM -> StrOutputParser\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en {tema}. Responde de forma {estilo}.\"),\n",
    "    (\"human\", \"{pregunta}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "print(\"Chain creada: PromptTemplate -> LLM -> StrOutputParser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta de la Chain:\n",
      "¡Claro que sí! Desplegar modelos de Deep Learning en Kubernetes (k8s) es un tema fascinante y muy potente. Imagina que tienes un modelo de inteligencia artificial súper chulo, entrenado para reconocer gatos en fotos, y quieres que esté disponible para que muchas aplicaciones puedan usarlo al instante, sin importar cuántas peticiones le lleguen. Kubernetes es como el \"director de orquesta\" que se encarga de que esto funcione de maravilla.\n",
      "\n",
      "Vamos a desglosarlo de forma sencilla, como si estuviéramos construyendo un pequeño \"equipo\" para nuestro modelo.\n",
      "\n",
      "### ¿Por qué usar Kubernetes para modelos de Deep Learning?\n",
      "\n",
      "Antes de meternos en faena, pensemos por qué es tan buena idea:\n",
      "\n",
      "*   **Escalabilidad:** Si de repente miles de personas quieren usar tu modelo para reconocer gatos, Kubernetes puede crear más copias de tu modelo automáticamente para atender toda esa demanda. ¡Adiós a los cuellos de botella!\n",
      "*   **Alta Disponibilidad:** Si una de las copias de tu modelo falla (imagina que se le acaba la \"energía\" o hay un problema), Kubernetes se da cuenta y lanza otra nueva al instante. Tu modelo siempre estará disponible.\n",
      "*   **Gestión de Recursos:** Los modelos de Deep Learning suelen necesitar mucha potencia (CPU y, sobre todo, GPUs). Kubernetes sabe cómo asignar estos recursos de forma eficiente a cada modelo, para que no se desperdicien.\n",
      "*   **Facilidad de Actualización:** ¿Tienes una versión mejorada de tu modelo? Kubernetes te permite actualizarlo gradualmente, sin que los usuarios noten interrupciones. Es como cambiar una pieza de un motor mientras sigue funcionando.\n",
      "*   **Portabilidad:** Tu modelo funcionará igual en tu ordenador, en la nube de Google, en la de Amazon, o en tu propio centro de datos. Kubernetes es un estándar.\n",
      "\n",
      "### Los \"Ingredientes\" que necesitamos en Kubernetes\n",
      "\n",
      "Para desplegar un modelo de Deep Learning, vamos a necesitar varios componentes de Kubernetes que trabajan juntos:\n",
      "\n",
      "1.  **Contenedores (Docker): El \"Cuerpo\" del Modelo**\n",
      "\n",
      "    *   **¿Qué es?** Piensa en un contenedor como una caja mágica que empaqueta todo lo que tu modelo necesita para funcionar: el código del modelo, las librerías (TensorFlow, PyTorch, etc.), los pesos del modelo entrenado, e incluso el sistema operativo mínimo.\n",
      "    *   **¿Por qué?** Esto asegura que tu modelo funcione exactamente igual en cualquier lugar donde se ejecute el contenedor, sin importar el entorno. ¡Adiós al clásico \"en mi máquina funciona\"!\n",
      "    *   **¿Cómo lo hacemos?** Crearemos un `Dockerfile` que describe cómo construir esta caja.\n",
      "\n",
      "2.  **Imágenes de Contenedor: La \"Receta\" para la Caja**\n",
      "\n",
      "    *   **¿Qué es?** Una vez que el `Dockerfile` está listo, lo usamos para construir una \"imagen\". Piensa en la imagen como el molde o la receta exacta para crear muchas cajas (contenedores) idénticas.\n",
      "    *   **¿Dónde la guardamos?** Subimos esta imagen a un \"Registro de Contenedores\" (como Docker Hub, Google Container Registry, AWS ECR, etc.). Es como un gran almacén de moldes.\n",
      "\n",
      "3.  **Pods: La \"Unidad Básica de Ejecución\"**\n",
      "\n",
      "    *   **¿Qué es?** En Kubernetes, no ejecutamos contenedores directamente. Los ejecutamos dentro de \"Pods\". Un Pod es la unidad más pequeña que Kubernetes puede desplegar y gestionar. Puede contener uno o varios contenedores relacionados. Para un modelo de DL, solemos tener un Pod con un contenedor principal que ejecuta el modelo.\n",
      "    *   **¿Por qué?** Los Pods nos dan más flexibilidad. Por ejemplo, podrías tener un Pod con el contenedor de tu modelo y otro contenedor auxiliar que se encargue de monitorizarlo o de hacer logging.\n",
      "\n",
      "4.  **Deployments: El \"Director de Orquesta\" para los Pods**\n",
      "\n",
      "    *   **¿Qué es?** Aquí es donde entra Kubernetes de verdad. Un `Deployment` le dice a Kubernetes: \"Quiero que siempre haya X copias de este Pod (que contiene mi modelo) funcionando\".\n",
      "    *   **¿Qué hace?**\n",
      "        *   **Crear Pods:** Se encarga de crear los Pods que definimos.\n",
      "        *   **Escalar:** Si le dices \"quiero 2 copias\" y luego \"quiero 5\", el Deployment se encarga de crear las 3 copias extra. Si una copia falla, el Deployment se da cuenta y crea una nueva.\n",
      "        *   **Actualizaciones:** Cuando quieres actualizar tu modelo, creas una nueva imagen de contenedor. Le dices al Deployment que use\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = chain.invoke({\n",
    "    \"tema\": \"Kubernetes\",\n",
    "    \"estilo\": \"Divulgativo\",\n",
    "    \"pregunta\": \"Explicame como puedo desplegar modelos de Deep Learning en un cluster the k8s\"\n",
    "})\n",
    "\n",
    "print(\"Respuesta de la Chain:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparación de Diferentes Prompts\n",
    "\n",
    "Ejecutamos la misma pregunta con diferentes system prompts para comparar en MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Versión: basico\n",
      "==================================================\n",
      "Docker es una **plataforma de código abierto** que se utiliza para **desarrollar, enviar y ejecutar aplicaciones en contenedores**.\n",
      "\n",
      "Piensa en un contenedor como una **caja ligera y portátil** que empaqueta todo lo que una aplicación necesita para ejecutarse: el código, las bibliotecas, las herramie...\n",
      "\n",
      "==================================================\n",
      "Versión: experto\n",
      "==================================================\n",
      "¡Excelente pregunta! Docker es una herramienta fundamental en el mundo DevOps y su impacto ha sido enorme. Permíteme explicártelo de forma técnica pero clara.\n",
      "\n",
      "En esencia, **Docker es una plataforma para desarrollar, enviar y ejecutar aplicaciones en contenedores.**\n",
      "\n",
      "Vamos a desglosar esto:\n",
      "\n",
      "### ¿Qu...\n",
      "\n",
      "==================================================\n",
      "Versión: estructurado\n",
      "==================================================\n",
      "¡Excelente pregunta! Como experto en DevOps, te explico qué es Docker de la siguiente manera:\n",
      "\n",
      "### 1) Definición\n",
      "\n",
      "**Docker es una plataforma de código abierto que permite automatizar el despliegue, la escalabilidad y la gestión de aplicaciones utilizando tecnologías de virtualización de contenedores...\n"
     ]
    }
   ],
   "source": [
    "# Diferentes versiones de prompts para comparar\n",
    "prompt_versions = [\n",
    "    {\n",
    "        \"name\": \"basico\",\n",
    "        \"system\": \"Eres un asistente útil.\",\n",
    "        \"user\": \"¿Qué es Docker?\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"experto\",\n",
    "        \"system\": \"Eres un experto en DevOps con 10 años de experiencia. Responde de forma técnica pero accesible.\",\n",
    "        \"user\": \"¿Qué es Docker?\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"estructurado\",\n",
    "        \"system\": \"Eres un experto en DevOps. Responde usando: 1) Definición, 2) Casos de uso, 3) Ventajas.\",\n",
    "        \"user\": \"¿Qué es Docker?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for version in prompt_versions:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Versión: {version['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=version[\"system\"]),\n",
    "        HumanMessage(content=version[\"user\"])\n",
    "    ]\n",
    "    \n",
    "response = llm.invoke(messages)\n",
    "print(response.content[:300] + \"...\")\n",
    "    mlflow.log_metric(\"response_length_chars\", len(response.content))\n",
    "    mlflow.log_metric(\"response_length_tokens_est\", len(response.content.split()))\n",
    "\n",
    "    # Log response as artifact (best practice)\n",
    "    response_path = f\"response_{version['name']}.txt\"\n",
    "    with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    mlflow.log_artifact(response_path)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Versión: {version['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(response.content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 3: Logging Manual Adicional\n",
    "\n",
    "Además del tracing automático, podemos añadir métricas y artifacts manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def experiment_with_run(run_name, system_prompt, user_prompt, temperature=0.7):\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "        mlflow.log_param(\"model\", \"\")\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        mlflow.log_param(\"system_prompt\", system_prompt[:200])\n",
    "        mlflow.log_param(\"user_prompt\", user_prompt[:200])\n",
    "        \n",
    "        # Creamos modelo con temperatura específica\n",
    "        llm_temp = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Medimos latencia\n",
    "        start = time.time()\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_prompt)\n",
    "        ]\n",
    "        response = llm_temp.invoke(messages)\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        \n",
    "        # Loggeamos métricas\n",
    "        mlflow.log_metric(\"latency_seconds\", latency)\n",
    "        mlflow.log_metric(\"response_length\", len(response.content))\n",
    "        \n",
    "        mlflow.log_text(response.content, \"response.txt\")\n",
    "        \n",
    "        mlflow.log_dict({\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"response\": response.content,\n",
    "            \"latency\": latency\n",
    "        }, \"interaction.json\")\n",
    "        \n",
    "        print(f\"Run: {run_name}\")\n",
    "        print(f\"Latencia: {latency:.2f}s\")\n",
    "        print(f\"Respuesta: {response.content[:150]}...\")\n",
    "        \n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_experiment(\"T\")\n",
    "\n",
    "temperatures = [0.0, 0.5, 1.0]\n",
    "prompt = \"\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    experiment_with_run(\n",
    "        run_name=f\"temp_{temp}\",\n",
    "        system_prompt=\"\",\n",
    "        user_prompt=prompt,\n",
    "        temperature=temp\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 4: Tracing Manual con Decorador\n",
    "\n",
    "Podemos usar `@mlflow.trace` para crear spans personalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace\n",
    "def generate_summary(text: str, max_words: int = 50) -> str:\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"Resume el siguiente texto en máximo {max_words} palabras.\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\"text\": text})\n",
    "\n",
    "\n",
    "@mlflow.trace\n",
    "def analyze_and_summarize(text: str) -> dict:\n",
    "    summary = generate_summary(text, max_words=30)\n",
    "    \n",
    "    return {\n",
    "        \"original_length\": len(text),\n",
    "        \"summary\": summary,\n",
    "        \"summary_length\": len(summary)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Custom_Tracin\")\n",
    "\n",
    "texto = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "result = analyze_and_summarize(texto)\n",
    "print(f\"Texto original: {result['original_length']} caracteres\")\n",
    "print(f\"Resumen: {result['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 5: Evaluación de Respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset de evaluación\n",
    "eval_data = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"¿Qué es un contenedor en Docker?\",\n",
    "        \"¿Para qué sirve Kubernetes?\",\n",
    "        \"¿Qué es CI/CD?\"\n",
    "    ],\n",
    "    \"expected_keywords\": [\n",
    "        [\"software\", \"dependencias\", \"aislado\"],\n",
    "        [\"orquestación\", \"contenedores\", \"escalado\"],\n",
    "        [\"integración\", \"continua\", \"despliegue\"]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"evaluation_run\"):\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in eval_data.iterrows():\n",
    "        start = time.time()\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"\"),\n",
    "            HumanMessage(content=row[\"question\"])\n",
    "        ]\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        \n",
    "        response_lower = response.content.lower()\n",
    "        keywords_found = sum(1 for kw in row[\"expected_keywords\"] if kw in response_lower)\n",
    "        keyword_score = keywords_found / len(row[\"expected_keywords\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": row[\"question\"],\n",
    "            \"response\": response.content,\n",
    "            \"latency\": latency,\n",
    "            \"keyword_score\": keyword_score\n",
    "        })\n",
    "        \n",
    "        mlflow.log_metric(f\"latency_q{idx}\", latency)\n",
    "        mlflow.log_metric(f\"keyword_score_q{idx}\", keyword_score)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    mlflow.log_metric(\"avg_latency\", results_df[\"latency\"].mean())\n",
    "    mlflow.log_metric(\"avg_keyword_score\", results_df[\"keyword_score\"].mean())\n",
    "    \n",
    "    mlflow.log_table(results_df, \"evaluation_results.json\")\n",
    "    \n",
    "    print(\"Resultados de evaluación:\")\n",
    "    print(results_df[[\"question\", \"latency\", \"keyword_score\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
